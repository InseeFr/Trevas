{"searchDocs":[{"title":"Trevas - Java 17","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/blog/trevas-java-17","content":"","keywords":"","version":null},{"title":"Trevas - Version 2.0.0","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/blog/trevas-2","content":"Trevas 2.0.0 is released! Following the implementation of DAGs and the reordering of VTL instructions before execution, evaluating a VTL script will integrate this new functionality by default. A technical documentation is available to describe this feature and how to disable it.","keywords":"","version":null},{"title":"News​","type":1,"pageTitle":"Trevas - Java 17","url":"/Trevas/zh-CN/blog/trevas-java-17#news","content":" Trevas 1.2.0 enables Java 17 support.  ","version":null,"tagName":"h3"},{"title":"Java modules handling​","type":1,"pageTitle":"Trevas - Java 17","url":"/Trevas/zh-CN/blog/trevas-java-17#java-modules-handling","content":" Spark does not support Java modules.  Java 17 client apps, embedding Trevas in Spark mode have to configure UNNAMED modules for Spark.  Maven​  Add to your pom.xml file, in the build &gt; plugins section:  &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.11.0&lt;/version&gt; &lt;configuration&gt; &lt;compilerArgs&gt; &lt;arg&gt;--add-exports&lt;/arg&gt; &lt;arg&gt;java.base/sun.nio.ch=ALL-UNNAMED&lt;/arg&gt; &lt;/compilerArgs&gt; &lt;/configuration&gt; &lt;/plugin&gt;   Docker​  ENTRYPOINT [&quot;java&quot;, &quot;--add-exports&quot;, &quot;java.base/sun.nio.ch=ALL-UNNAMED&quot;, &quot;mainClass&quot;]  ","version":null,"tagName":"h3"},{"title":"Trevas Batch 0.1.1","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/blog/trevas-batch-0.1.1","content":"","keywords":"","version":null},{"title":"Launch​","type":1,"pageTitle":"Trevas Batch 0.1.1","url":"/Trevas/zh-CN/blog/trevas-batch-0.1.1#launch","content":" Local​  java -jar trevas-batch-0.1.1.jar -Dconfig.path=&quot;...&quot; -Dreport.path=&quot;...&quot;   The java execution will be done in local Spark.  Kubernetes​  Default Kubernetes objects are defined in the .kubernetes folder.  Feed the config-map.yml file then launch the job in your cluster. ","version":null,"tagName":"h3"},{"title":"Trevas - Persistent assignments","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/blog/trevas-persistent-assignments","content":"","keywords":"","version":null},{"title":"News​","type":1,"pageTitle":"Trevas - Persistent assignments","url":"/Trevas/zh-CN/blog/trevas-persistent-assignments#news","content":" Trevas 1.2.0 includes the persistent assignment support: ds1 &lt;- ds;.  In Trevas, persistent datasets are represented as PersistentDataset.  ","version":null,"tagName":"h3"},{"title":"Handle PersistentDataset​","type":1,"pageTitle":"Trevas - Persistent assignments","url":"/Trevas/zh-CN/blog/trevas-persistent-assignments#handle-persistentdataset","content":" Trevas datasets are represented as Dataset.  After running the Trevas engine, you can use persistent datasets with something like:  Bindings engineBindings = engine.getContext().getBindings(ScriptContext.ENGINE_SCOPE); engineBindings.forEach((k, v) -&gt; { if (v instanceof PersistentDataset) { fr.insee.vtl.model.Dataset ds = ((PersistentDataset) v).getDelegate(); if (ds instanceof SparkDataset) { Dataset&lt;Row&gt; sparkDs = ((SparkDataset) ds).getSparkDataset(); // Do what you want with sparkDs } } });  ","version":null,"tagName":"h3"},{"title":"Trevas - check_hierarchy","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/blog/trevas-check_hierarchy","content":"","keywords":"","version":null},{"title":"News​","type":1,"pageTitle":"Trevas - check_hierarchy","url":"/Trevas/zh-CN/blog/trevas-check_hierarchy#news","content":" Trevas 1.1.0 includes hierarchical validation via operators define hierarchical ruleset and check_hierarchy.  ","version":null,"tagName":"h3"},{"title":"Example​","type":1,"pageTitle":"Trevas - check_hierarchy","url":"/Trevas/zh-CN/blog/trevas-check_hierarchy#example","content":" Input​  ds1:  id\tMeABC\t12 A\t1 B\t10 C\t1 DEF\t100 E\t99 F\t1 HIJ\t100 H\t99 I\t0  VTL script​  // Ensure ds1 metadata definition is good ds1 := ds1[calc identifier id := id, Me := cast(Me, integer)]; // Define hierarchical ruleset define hierarchical ruleset hr (variable rule Me) is My_Rule : ABC = A + B + C errorcode &quot;ABC is not sum of A,B,C&quot; errorlevel 1; DEF = D + E + F errorcode &quot;DEF is not sum of D,E,F&quot;; HIJ : HIJ = H + I - J errorcode &quot;HIJ is not H + I - J&quot; errorlevel 10 end hierarchical ruleset; // Check hierarchy ds_all := check_hierarchy(ds1, hr rule id all); ds_all_measures := check_hierarchy(ds1, hr rule id always_null all_measures); ds_invalid := check_hierarchy(ds1, hr rule id always_zero invalid);   Outputs​  ds_all  id\truleid\tbool_var\terrorcode\terrorlevel\timbalanceABC\tMy_Rule\ttrue\tnull\tnull\t0  ds_always_null_all_measures  id\tMe\truleid\tbool_var\terrorcode\terrorlevel\timbalanceABC\t12\tMy_Rule\ttrue\tnull\tnull\t0 DEF\t100\thr_2\tnull\tnull\tnull\tnull HIJ\t100\tHIJ\tnull\tnull\tnull\tnull  ds_invalid  id\tMe\truleid\terrorcode\terrorlevel\timbalanceHIJ\t100\tHIJ\tHIJ is not H + I - J\t10\t1 ","version":null,"tagName":"h3"},{"title":"Trevas Jupyter 0.3.2","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/blog/trevas-jupyter-0.3.2","content":"","keywords":"","version":null},{"title":"News​","type":1,"pageTitle":"Trevas Jupyter 0.3.2","url":"/Trevas/zh-CN/blog/trevas-jupyter-0.3.2#news","content":" In addition to the VTL coverage greatly increased since the publication of Trevas 1.x.x, Trevas Jupyter offers 1 new connector:  SAS files (via the loadSas method)  ","version":null,"tagName":"h3"},{"title":"Launch​","type":1,"pageTitle":"Trevas Jupyter 0.3.2","url":"/Trevas/zh-CN/blog/trevas-jupyter-0.3.2#launch","content":" Manually adding the Trevas Kernel to an existing Jupyter instance​  Trevas Jupyter compilerCopy the kernel.json file and the bin and repo folders to a new kernel folder.Edit the kernel.json fileLaunch Jupyter  Docker​  docker pull inseefrlab/trevas-jupyter:0.3.2 docker run -p 8888:8888 inseefrlab/trevas-jupyter:0.3.2   Helm​  The Trevas Jupyter docker image can be instantiated via the jupyter-pyspark Helm contract from InseeFrLab. ","version":null,"tagName":"h3"},{"title":"Trevas Lab 0.3.3","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/blog/trevas-lab-0.3.3","content":"","keywords":"","version":null},{"title":"News​","type":1,"pageTitle":"Trevas Lab 0.3.3","url":"/Trevas/zh-CN/blog/trevas-lab-0.3.3#news","content":" In addition to the VTL coverage greatly increased since the publication of Trevas 1.x.x, Trevas Lab offers 2 new connectors:  SAS filesJDBC MariaDB  ","version":null,"tagName":"h3"},{"title":"Launch​","type":1,"pageTitle":"Trevas Lab 0.3.3","url":"/Trevas/zh-CN/blog/trevas-lab-0.3.3#launch","content":" Kubernetes​  Sample Kubernetes objects are available in the .kubernetes folders of Trevas Lab and Trevas Lab UI. ","version":null,"tagName":"h3"},{"title":"Trevas - Temporal operators","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/blog/trevas-temporal-operators","content":"","keywords":"","version":null},{"title":"Temporal operators in Trevas​","type":1,"pageTitle":"Trevas - Temporal operators","url":"/Trevas/zh-CN/blog/trevas-temporal-operators#temporal-operators-in-trevas","content":" The version 1.4.1 of Trevas introduces preliminary support for date and time types and operators.  The specification describes temporal types such as date, time_period, time, and duration. However, Trevas authors find these descriptions unsatisfactory. This blog post outlines our implementation choices and how they differ from the spec.  In the specification, time_period (and the types date) is described as a compound type with a start and end (or a start and a duration). This complicates the implementation and brings little value to the language as one can simply operate on a combination of dates or date and duration directly. For this reason, we defined an algebra between the temporal types and did not yet implement the time_period.  result (operators)\tdate\tduration\tnumberdate\tn/a\tdate (+, -)\tn/a duration\tdate (+, -)\tduration (+, -)\tduration (*) number\tn/a\tduration (*)\tn/a  The period_indicator function relies on period-awareness for types that are not defined enough at the moment to be implemented.  Java mapping​  The VTL type date is represented internally as the types java.time.Instant,java.time.ZonedDateTimeand java.time.OffsetDateTime  Instant represent a specific moment in time. Note that this type does not include timezone information and is therefore not usable with all the operators. One can use the types ZonedDateTime and OffsetDateTime when timezone or time saving is required.  The VTL type duration is represented internally as the type org.threeten.extra.PeriodDurationfrom the threeten extra package. It represents a duration using both calendar units (years, months, days) and a temporal amount (hours, minutes, seconds and nanoseconds).  Function flow_to_stock​  The flow_to_stock function converts a data set with flow interpretation into a stock interpretation. This transformation is useful when you want to aggregate flow data (e.g., sales or production rates) into cumulative stock data (e.g., total inventory).  Syntax:  result := flow_to_stock(op)  Parameters:  op - The input data set with flow interpretation. The data set must have an identifier of type time, additional identifiers, and at least one measure of type number.  Result:  The function returns a data set with the same structure as the input, but with the values converted to stock interpretation.  Function stock_to_flow​  The stock_to_flow function converts a data set with stock interpretation into a flow interpretation. This transformation is useful when you want to derive flow data from cumulative stock data.  Syntax:  result := stock_to_flow(op)  Parameters:  op - The input data set with stock interpretation. The data set must have an identifier of type time, additional identifiers, and at least one measure of type number.  Result:  The function returns a data set with the same structure as the input, but with the values converted to flow interpretation.  Function timeshift​  The timeshift function shifts the time component of a specified range of time in the data set. This is useful for analyzing data at different time offsets, such as comparing current values to past values.  Syntax:  result := timeshift(op, shiftNumber)  Parameters:  op - The operand data set containing time series.shiftNumber - An integer representing the number of periods to shift. Positive values shift forward in time, while negative values shift backward.  Result:  The function returns a data set with the time identifiers shifted by the specified number of periods. ","version":null,"tagName":"h3"},{"title":"Trevas - VTL 2.1","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/blog/trevas-vtl-21","content":"Trevas 1.7.0 upgrade to version 2.1 of VTL. This version introduces two new operators: randomcase random produces a decimal number between 0 and 1. case allows for clearer multi conditional branching, for example: ds2 := ds1[ calc c := case when r &lt; 0.2 then &quot;Low&quot; when r &gt; 0.8 then &quot;High&quot; else &quot;Medium&quot; ] Both operators are already available in Trevas! The new grammar also provides time operators and includes corrections, without any breaking changes compared to the 2.0 version. See the coverage section for more details.","keywords":"","version":null},{"title":"Trevas - Provenance","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/blog/trevas-provenance","content":"","keywords":"","version":null},{"title":"News​","type":1,"pageTitle":"Trevas - Provenance","url":"/Trevas/zh-CN/blog/trevas-provenance#news","content":" Trevas 1.6.0 introduces the VTL Prov module.  This module enables to produce lineage metadata from Trevas, based on RDF ontologies: PROV-O and SDTH.  SDTH model overview​    Adopted model​  The vtl-prov module, version 1.6.0, uses the following partial model:    Improvements will come in next weeks.  Tools available​  Provenance Trevas tools are documented here.  Example​  Business use case​  Two sources datasets are transformed to produce transient datasets and a final permanent one.    ","version":null,"tagName":"h3"},{"title":"Inputs​","type":1,"pageTitle":"Trevas - Provenance","url":"/Trevas/zh-CN/blog/trevas-provenance#inputs","content":" ds1 &amp; ds2 metadata:  id\tvar1\tvar2STRING\tINTEGER\tNUMBER IDENTIFIER\tMEASURE\tMEASURE  ","version":null,"tagName":"h3"},{"title":"VTL script​","type":1,"pageTitle":"Trevas - Provenance","url":"/Trevas/zh-CN/blog/trevas-provenance#vtl-script","content":" ds_sum := ds1 + ds2; ds_mul := ds_sum * 3; ds_res &lt;- ds_mul[filter mod(var1, 2) = 0][calc var_sum := var1 + var2];   ","version":null,"tagName":"h3"},{"title":"RDF model target​","type":1,"pageTitle":"Trevas - Provenance","url":"/Trevas/zh-CN/blog/trevas-provenance#rdf-model-target","content":" PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; PREFIX prov: &lt;http://www.w3.org/ns/prov#&gt; PREFIX sdth: &lt;http://rdf-vocabulary.ddialliance.org/sdth#&gt; # --- Program and steps &lt;http://example.com/program1&gt; a sdth:Program ; a prov:Agent ; # Agent? Or an activity rdfs:label &quot;My program 1&quot;@en, &quot;Mon programme 1&quot;@fr ; sdth:hasProgramStep &lt;http://example.com/program1/program-step1&gt;, &lt;http://example.com/program1/program-step2&gt;, &lt;http://example.com/program1/program-step3&gt; . &lt;http://example.com/program1/program-step1&gt; a sdth:ProgramStep ; rdfs:label &quot;Program step 1&quot;@en, &quot;Étape 1&quot;@fr ; sdth:hasSourceCode &quot;ds_sum := ds1 + ds2;&quot; ; sdth:consumesDataframe &lt;http://example.com/dataset/ds1&gt;, &lt;http://example.com/dataset/ds2&gt; ; sdth:producesDataframe &lt;http://example.com/dataset/ds_sum&gt; . &lt;http://example.com/program1/program-step2&gt; a sdth:ProgramStep ; rdfs:label &quot;Program step 2&quot;@en, &quot;Étape 2&quot;@fr ; sdth:hasSourceCode &quot;ds_mul := ds_sum * 3;&quot; ; sdth:consumesDataframe &lt;http://example.com/dataset/ds_sum&gt; ; sdth:producesDataframe &lt;http://example.com/dataset/ds_mul&gt; . &lt;http://example.com/program1/program-step3&gt; a sdth:ProgramStep ; rdfs:label &quot;Program step 3&quot;@en, &quot;Étape 3&quot;@fr ; sdth:hasSourceCode &quot;ds_res &lt;- ds_mul[filter mod(var1, 2) = 0][calc var_sum := var1 + var2];&quot; ; sdth:consumesDataframe &lt;http://example.com/dataset/ds_mul&gt; ; sdth:producesDataframe &lt;http://example.com/dataset/ds_res&gt; ; sdth:usesVariable &lt;http://example.com/variable/var1&gt;, &lt;http://example.com/variable/var2&gt; ; sdth:assignsVariable &lt;http://example.com/variable/var_sum&gt; . # --- Variables # i think here it's not instances but names we refer to... &lt;http://example.com/variable/id1&gt; a sdth:VariableInstance ; rdfs:label &quot;id1&quot; . &lt;http://example.com/variable/var1&gt; a sdth:VariableInstance ; rdfs:label &quot;var1&quot; . &lt;http://example.com/variable/var2&gt; a sdth:VariableInstance ; rdfs:label &quot;var2&quot; . &lt;http://example.com/variable/var_sum&gt; a sdth:VariableInstance ; rdfs:label &quot;var_sum&quot; . # --- Data frames &lt;http://example.com/dataset/ds1&gt; a sdth:DataframeInstance ; rdfs:label &quot;ds1&quot; ; sdth:hasName &quot;ds1&quot; ; sdth:hasVariableInstance &lt;http://example.com/variable/id1&gt;, &lt;http://example.com/variable/var1&gt;, &lt;http://example.com/variable/var2&gt; . &lt;http://example.com/dataset/ds2&gt; a sdth:DataframeInstance ; rdfs:label &quot;ds2&quot; ; sdth:hasName &quot;ds2&quot; ; sdth:hasVariableInstance &lt;http://example.com/variable/id1&gt;, &lt;http://example.com/variable/var1&gt;, &lt;http://example.com/variable/var2&gt; . &lt;http://example.com/dataset/ds_sum&gt; a sdth:DataframeInstance ; rdfs:label &quot;ds_sum&quot; ; sdth:hasName &quot;ds_sum&quot; ; sdth:wasDerivedFrom &lt;http://example.com/dataset/ds1&gt;, &lt;http://example.com/dataset/ds2&gt; ; sdth:hasVariableInstance &lt;http://example.com/variable/id1&gt;, &lt;http://example.com/variable/var1&gt;, &lt;http://example.com/variable/var2&gt; . &lt;http://example.com/dataset/ds_mul&gt; a sdth:DataframeInstance ; rdfs:label &quot;ds_mul&quot; ; sdth:hasName &quot;ds_mul&quot; ; sdth:wasDerivedFrom &lt;http://example.com/dataset/ds_sum&gt; ; sdth:hasVariableInstance &lt;http://example.com/variable/id1&gt;, &lt;http://example.com/variable/var1&gt;, &lt;http://example.com/variable/var2&gt; . &lt;http://example.com/dataset/ds_res&gt; a sdth:DataframeInstance ; rdfs:label &quot;ds_res&quot; ; sdth:wasDerivedFrom &lt;http://example.com/dataset/ds_mul&gt; ; sdth:hasVariableInstance &lt;http://example.com/variable/id1&gt;, &lt;http://example.com/variable/var1&gt;, &lt;http://example.com/variable/var2&gt;, &lt;http://example.com/variable/var_sum&gt; .  ","version":null,"tagName":"h3"},{"title":"Trevas - SDMX","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/blog/trevas-sdmx","content":"","keywords":"","version":null},{"title":"News​","type":1,"pageTitle":"Trevas - SDMX","url":"/Trevas/zh-CN/blog/trevas-sdmx#news","content":" Trevas 1.4.1 introduces the VTL SDMX module.  This module enables to consume SDMX metadata sources to instantiate Trevas DataStructures and Datasets.  It also allows to execute the VTL TransformationSchemes to obtain the resulting persistent datasets.  Overview​    Trevas supports the above SDMX message elements. Only the VtlMappingSchemes element is optional.  The elements in box 1 are used to produce Trevas DataStructures, filling VTL components attributes name, role, type, nullable and valuedomain.  The elements in box 2 are used to generate the VTL code (rulesets &amp; transformations).  Tools available​  SDMX Trevas tools are documented here.  Troubleshooting​  Have a look to this section. ","version":null,"tagName":"h3"},{"title":"VTL语法的修改","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/administrator-guide/grammar","content":"","keywords":"","version":"下一个"},{"title":"Utilisation de la grammaire VTL​","type":1,"pageTitle":"VTL语法的修改","url":"/Trevas/zh-CN/administrator-guide/grammar#utilisation-de-la-grammaire-vtl","content":" Trevas s'appuie sur la grammaire formelle de VTL exprimée sous forme EBNF. La référence est l'upgrade de la version 2.1 publiée en juillet 2024 sur le site web SDMX.  La grammaire consiste en deux fichiers prêts à être traités par le générateur de parseurs Antlr :  VtlTokens.g4 contient la liste des termes VTL valides. Vtl.g4 contient les règles qui créent les expressions VTL valides.  Antlr utilise ces fichiers pour produire un lexeur, qui crée une liste de symboles du vocabulaire à partir d'un flot de caractères en entrée, et un parseur, qui crée la structure grammaticale correspondant à cette liste de symboles. Antlr peut générer des parseurs utilisables dans différents langages cibles. Trevas utilise le parseur pour Java, qui est exposé dans le module vtl-parser.  ","version":"下一个","tagName":"h2"},{"title":"Adaptations de la grammaire​","type":1,"pageTitle":"VTL语法的修改","url":"/Trevas/zh-CN/administrator-guide/grammar#adaptations-de-la-grammaire","content":" Afin d'améliorer les performances et les fonctionnalités, des modifications mineures ont été faites à la grammaire VTL grammar utilisée dans Trevas.  Simplification de l'arbre grammatical​  Comme documenté ici et ici, les branches expr et exprComp de l'arbre grammatical sont presques identiques. Afin d'éviter d'avoir à implémenter deux fois la même logique, la branche exprComp a été mise en commentaires par le commit 498c1f8. Il fut remarqué par la suite que cette modification invalidait à tort l'expression COUNT() expression, et la règle correspondante fut donc réintroduite dans la grammaire par le commit [54f86f2] (https://github.com/InseeFr/Trevas/commit/54f86f27d2e8fdd57df1439d74ed56d225064a7d).  Addition d'opérateurs de distance​  Les opérateurs de distances tels que Levenshtein ou Jaro-Winkler sont communément utilisés dans les tests sur les chaînes de caractères. Afin de les autoriser dans les expressions VTL, le commit 036dc60 a ajouté dans la grammaire une section distanceOperators contenant une règle LEVENSHTEIN, ainsi que le symbole LEVENSHTEIN dans le fichier du lexeur. ","version":"下一个","tagName":"h3"},{"title":"基本模式 - 数据源","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/developer-guide/basic-mode/data-sources","content":"","keywords":"","version":"下一个"},{"title":"Bonnes pratiques Trevas​","type":1,"pageTitle":"基本模式 - 数据源","url":"/Trevas/zh-CN/developer-guide/basic-mode/data-sources#bonnes-pratiques-trevas","content":"  Le format JSON est le seul permettant de stocker et gérer les métadonnées VTL lorsque le moteur Trevas est instancié en mode de base.   Il est donc fortement conseillé d'utiliser ce format.  JDBC JSON Others ","version":"下一个","tagName":"h3"},{"title":"Developer guide","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/developer-guide","content":"","keywords":"","version":"下一个"},{"title":"Importer le moteur Trevas​","type":1,"pageTitle":"Developer guide","url":"/Trevas/zh-CN/developer-guide#importer-le-moteur-trevas","content":" &lt;dependency&gt; &lt;groupId&gt;fr.insee.trevas&lt;/groupId&gt; &lt;artifactId&gt;vtl-engine&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt; &lt;/dependency&gt;   ","version":"下一个","tagName":"h3"},{"title":"Instancier le moteur Trevas​","type":1,"pageTitle":"Developer guide","url":"/Trevas/zh-CN/developer-guide#instancier-le-moteur-trevas","content":" // Start engine ScriptEngine engine = new ScriptEngineManager().getEngineByName(&quot;vtl&quot;); // Add input bindings to engine Bindings bindings = new SimpleBindings(); bindings.put(&quot;a&quot;, 1); engine.setBindings(b, ScriptContext.ENGINE_SCOPE); // Execute script try { engine.eval(&quot;b := a + 1;&quot;); } catch (VtlScriptException e) { logger.warn(&quot;Eval failed: &quot;, e); } // Get result Long result = (Long) engine.getBindings(ScriptContext.ENGINE_SCOPE).get(&quot;b&quot;);   ","version":"下一个","tagName":"h3"},{"title":"Mode d'exécution​","type":1,"pageTitle":"Developer guide","url":"/Trevas/zh-CN/developer-guide#mode-dexécution","content":" Basic mode Spark mode ","version":"下一个","tagName":"h3"},{"title":"基本模式","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/developer-guide/basic-mode","content":"","keywords":"","version":"下一个"},{"title":"InMemoryDataset​","type":1,"pageTitle":"基本模式","url":"/Trevas/zh-CN/developer-guide/basic-mode#inmemorydataset","content":" Les datasets InMemoryDataset permettent de représenter les tables statistiques dans une application Java.  ","version":"下一个","tagName":"h3"},{"title":"Exemple​","type":1,"pageTitle":"基本模式","url":"/Trevas/zh-CN/developer-guide/basic-mode#exemple","content":" ScriptEngine engine = new ScriptEngineManager().getEngineByName(&quot;vtl&quot;); Bindings bindings = new SimpleBindings(); InMemoryDataset dataset = new InMemoryDataset( List.of( Map.of(&quot;var1&quot;, &quot;x&quot;, &quot;var2&quot;, &quot;y&quot;, &quot;var3&quot;, 5), Map.of(&quot;var1&quot;, &quot;xx&quot;, &quot;var2&quot;, &quot;yy&quot;, &quot;var3&quot;, 10) ), Map.of(&quot;var1&quot;, String.class, &quot;var2&quot;, String.class, &quot;var3&quot;, Long.class), Map.of(&quot;var1&quot;, Role.IDENTIFIER, &quot;var2&quot;, Role.ATTRIBUTE, &quot;var3&quot;, Role.MEASURE) ); bindings.put(&quot;myDataset&quot;, dataset); engine.getContext().setBindings(bindings, ScriptContext.ENGINE_SCOPE); String script = &quot;res := myDataset[filter var3 &gt; 6];&quot;; try { engine.eval(script); } catch (ScriptException e) { e.printStackTrace(); } Bindings outputBindings = engine.getContext().getBindings(ScriptContext.ENGINE_SCOPE); InMemoryDataset result = (InMemoryDataset) outputBindings.get(&quot;res&quot;); System.out.println(result.getDataPoints().size()); // 1  ","version":"下一个","tagName":"h3"},{"title":"基本模式 - JDBC 数据源","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/developer-guide/basic-mode/data-sources/jdbc","content":"","keywords":"","version":"下一个"},{"title":"Importer le module JDBC de Trevas​","type":1,"pageTitle":"基本模式 - JDBC 数据源","url":"/Trevas/zh-CN/developer-guide/basic-mode/data-sources/jdbc#importer-le-module-jdbc-de-trevas","content":" &lt;dependency&gt; &lt;groupId&gt;fr.insee.trevas&lt;/groupId&gt; &lt;artifactId&gt;vtl-jdbc&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt; &lt;/dependency&gt;   ","version":"下一个","tagName":"h3"},{"title":"Utilisation du module vtl-jdbc​","type":1,"pageTitle":"基本模式 - JDBC 数据源","url":"/Trevas/zh-CN/developer-guide/basic-mode/data-sources/jdbc#utilisation-du-module-vtl-jdbc","content":" connection = DriverManager.getConnection(...); Statement statement = connection.createStatement(); JDBCDataset jdbcDataset = new JDBCDataset(() -&gt; { try { return statement.executeQuery(&quot;select * from ds1;&quot;); } catch (SQLException se) { throw new RuntimeException(se); } });  ","version":"下一个","tagName":"h3"},{"title":"基本模式 - JSON 数据源","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/developer-guide/basic-mode/data-sources/json","content":"","keywords":"","version":"下一个"},{"title":"Importer le module jackson de Trevas​","type":1,"pageTitle":"基本模式 - JSON 数据源","url":"/Trevas/zh-CN/developer-guide/basic-mode/data-sources/json#importer-le-module-jackson-de-trevas","content":" &lt;dependency&gt; &lt;groupId&gt;fr.insee.trevas&lt;/groupId&gt; &lt;artifactId&gt;vtl-jackson&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt; &lt;/dependency&gt;   ","version":"下一个","tagName":"h3"},{"title":"Schéma​","type":1,"pageTitle":"基本模式 - JSON 数据源","url":"/Trevas/zh-CN/developer-guide/basic-mode/data-sources/json#schéma","content":" La représentation JSON d'un Dataset est définie ainsi :  { &quot;dataStructure&quot;: [ { &quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;STRING&quot;, &quot;role&quot;: &quot;IDENTIFIER&quot; }, { &quot;name&quot;: &quot;x&quot;, &quot;type&quot;: &quot;INTEGER&quot;, &quot;role&quot;: &quot;MEASURE&quot; }, { &quot;name&quot;: &quot;y&quot;, &quot;type&quot;: &quot;FLOAT&quot;, &quot;role&quot;: &quot;MEASURE&quot; } ], &quot;dataPoints&quot;: [ [&quot;0001&quot;, 10, 50.5], [&quot;0002&quot;, 20, -8], [&quot;0003&quot;, 1000, 0], [&quot;0004&quot;, 1, 4.5] ] }   ","version":"下一个","tagName":"h3"},{"title":"Utilisation du module vtl-jackson​","type":1,"pageTitle":"基本模式 - JSON 数据源","url":"/Trevas/zh-CN/developer-guide/basic-mode/data-sources/json#utilisation-du-module-vtl-jackson","content":" Déclaration globale​  Le module peut être déclaré globalement à l'échelle du projet client.  public ObjectMapper objectMapper() { return new ObjectMapper() .registerModule(new TrevasModule()); }   Exemple de désérialisation​  ObjectMapper objectMapper = new ObjectMapper(); objectMapper.readValue(json, Dataset.class);  ","version":"下一个","tagName":"h3"},{"title":"Javadoc","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/developer-guide/javadoc","content":"","keywords":"","version":"下一个"},{"title":"Javadoc​","type":1,"pageTitle":"Javadoc","url":"/Trevas/zh-CN/developer-guide/javadoc#javadoc","content":" The Trevas Javadoc is available online thanks to https://javadoc.io/. ","version":"下一个","tagName":"h3"},{"title":"DAG (Directed acyclic graph) - Statement reordering","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/developer-guide/dag","content":"","keywords":"","version":"下一个"},{"title":"Statement reordering​","type":1,"pageTitle":"DAG (Directed acyclic graph) - Statement reordering","url":"/Trevas/zh-CN/developer-guide/dag#statement-reordering","content":" Suppose we have two Transformations:  Create an intermediate dataset DS_npUse DS_np to calculate another dataset DS_p  Even if we write them in &quot;reverse&quot; order, the VTL standard requires executing them in the correct dependency order.  -- Transformation Scheme Example -- (i) This depends on DS_np DS_p &lt;- if DS_np &gt;= 0 then DS_np else DS_1; -- (ii) This produces DS_np DS_np := (DS_1 - DS_2) * 2;   Execution Order (resolved by the engine)  (ii) must run first because DS_np is required before evaluating (i). (i) runs afterwards, since it consumes DS_np.  So even though we wrote (i) before (ii), the engine reorders them automatically, when reordering is activated.  ","version":"下一个","tagName":"h3"},{"title":"Activate reordering​","type":1,"pageTitle":"DAG (Directed acyclic graph) - Statement reordering","url":"/Trevas/zh-CN/developer-guide/dag#activate-reordering","content":" The current behavior of Trevas is that statement reordering is activated per default, as this is the behavior required by the VTL standard. Statement reordering can be deactivated via the following config flag (&quot;$vtl.engine.use_dag&quot;)  ScriptEngine engine = new ScriptEngineManager().getEngineByName(&quot;vtl&quot;); engine.put(&quot;$vtl.engine.use_dag&quot;, &quot;false&quot;);  ","version":"下一个","tagName":"h3"},{"title":"基本模式 - 其它数据源","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/developer-guide/basic-mode/data-sources/others","content":"","keywords":"","version":"下一个"},{"title":"Constructeur InMemoryDataset​","type":1,"pageTitle":"基本模式 - 其它数据源","url":"/Trevas/zh-CN/developer-guide/basic-mode/data-sources/others#constructeur-inmemorydataset","content":" InMemoryDataset dataset = new InMemoryDataset( List.of( Map.of(&quot;var1&quot;, &quot;x&quot;, &quot;var2&quot;, &quot;y&quot;, &quot;var3&quot;, 5), Map.of(&quot;var1&quot;, &quot;xx&quot;, &quot;var2&quot;, &quot;yy&quot;, &quot;var3&quot;, 10) ), Map.of(&quot;var1&quot;, String.class, &quot;var2&quot;, String.class, &quot;var3&quot;, Long.class), Map.of(&quot;var1&quot;, Role.IDENTIFIER, &quot;var2&quot;, Role.ATTRIBUTE, &quot;var3&quot;, Role.MEASURE) );  ","version":"下一个","tagName":"h3"},{"title":"Provenance","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/developer-guide/provenance","content":"","keywords":"","version":"下一个"},{"title":"Import Trevas Provenance module​","type":1,"pageTitle":"Provenance","url":"/Trevas/zh-CN/developer-guide/provenance#import-trevas-provenance-module","content":" &lt;dependency&gt; &lt;groupId&gt;fr.insee.trevas&lt;/groupId&gt; &lt;artifactId&gt;vtl-spark&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt; &lt;/dependency&gt;   ","version":"下一个","tagName":"h3"},{"title":"ProvenanceListener​","type":1,"pageTitle":"Provenance","url":"/Trevas/zh-CN/developer-guide/provenance#provenancelistener","content":" vtl-prov module expose the ProvenanceListener static class.  This class give access to the run function with the following signature:  run(String script, String programId, String programeName)   ","version":"下一个","tagName":"h3"},{"title":"Program​","type":1,"pageTitle":"Provenance","url":"/Trevas/zh-CN/developer-guide/provenance#program","content":" run function returns a Program object, containing all provenance information of a script.  Program program = ProvenanceListener.run(script, &quot;program-id&quot;, &quot;program-name&quot;);   ","version":"下一个","tagName":"h3"},{"title":"RDF​","type":1,"pageTitle":"Provenance","url":"/Trevas/zh-CN/developer-guide/provenance#rdf","content":" vtl-prov embeds jena package and expose RDFUtils static class.  Build RDF model​  RDFUtils buildModel method enables to build easily a RDF model of Program:  Model model = RDFUtils.buildModel(program);   RDF serialization​  RDFUtils buildModel enables to obtain easily a RDF serialization of Model:  String jsonLD = RDFUtils.serialize(model, &quot;JSON-LD&quot;); String ttl = RDFUtils.serialize(model, &quot;TTL&quot;); ...   Load in triple store​  RDFUrils loadModelWithCredentials enables to push easily RDF model in a triple store:  RDFUtils.loadModelWithCredentials(model, sparqlEndpoint, sparqlEndpointUser, sparlqEndpointPassword);  ","version":"下一个","tagName":"h3"},{"title":"Spark 模式 - JDBC数据源","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/jdbc","content":"","keywords":"","version":"下一个"},{"title":"读取 JDBC 数据源​","type":1,"pageTitle":"Spark 模式 - JDBC数据源","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/jdbc#读取-jdbc-数据源","content":" Dataset&lt;Row&gt; sparkDataset = spark.read() .format(&quot;jdbc&quot;) .option(&quot;url&quot;, &quot;myUrl&quot;) .option(&quot;user&quot;, &quot;myUser&quot;) .option(&quot;password&quot;, &quot;myPwd&quot;) .option(&quot;query&quot;, &quot;myQuery&quot;) .option(&quot;driver&quot;, &quot;org.postgresql.Driver&quot;) .load(); SparkDataset dataset = new SparkDataset(sparkDataset);   在下面的例子中，项目必须包含 postgresql Driver 的依赖  &lt;dependency&gt; &lt;groupId&gt;org.postgresql&lt;/groupId&gt; &lt;artifactId&gt;postgresql&lt;/artifactId&gt; &lt;version&gt;42.5.1&lt;/version&gt; &lt;/dependency&gt;   查看所有支持的选项 官方文档.  ","version":"下一个","tagName":"h3"},{"title":"写入​","type":1,"pageTitle":"Spark 模式 - JDBC数据源","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/jdbc#写入","content":" 不建议使用 jdbc 格式来编写 Trevas 的Dataset (请看 这里) ","version":"下一个","tagName":"h3"},{"title":"Spark 模式 - 数据源","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources","content":"","keywords":"","version":"下一个"},{"title":"Trevas 良好做法​","type":1,"pageTitle":"Spark 模式 - 数据源","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources#trevas-良好做法","content":"  当 Trevas 引擎在 Spark 模式下运行时， Apache Parquet  格式是唯一允许存储和管理 VTL 元数据的格式。   因此强烈建议使用Apache Parquet这种格式。  ","version":"下一个","tagName":"h3"},{"title":"例子​","type":1,"pageTitle":"Spark 模式 - 数据源","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources#例子","content":" Apache Parquet CSV JDBC SDMX Others ","version":"下一个","tagName":"h3"},{"title":"Spark模式 - CSV 数据源","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/csv","content":"","keywords":"","version":"下一个"},{"title":"读取 csv 数据源​","type":1,"pageTitle":"Spark模式 - CSV 数据源","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/csv#读取-csv-数据源","content":" Dataset&lt;Row&gt; sparkDataset = spark.read() .option(&quot;delimiter&quot;, &quot;;&quot;) .option(&quot;header&quot;, &quot;true&quot;) .csv(&quot;folder_path&quot;); SparkDataset dataset = new SparkDataset(sparkDataset);   查看所有支持的选项 官方文档.  ","version":"下一个","tagName":"h3"},{"title":"写入​","type":1,"pageTitle":"Spark模式 - CSV 数据源","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/csv#写入","content":" 不建议使用 CSV 格式来编写 Trevas Dataset (请看 这里) ","version":"下一个","tagName":"h3"},{"title":"Spark 模式 - 其它数据源","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/others","content":"","keywords":"","version":"下一个"},{"title":"Constructeur SparkDataset​","type":1,"pageTitle":"Spark 模式 - 其它数据源","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/others#constructeur-sparkdataset","content":" StructType schema = DataTypes.createStructType(List.of( DataTypes.createStructField(&quot;string&quot;, DataTypes.StringType, false), DataTypes.createStructField(&quot;integer&quot;, DataTypes.LongType, false), DataTypes.createStructField(&quot;boolean&quot;, DataTypes.BooleanType, false), DataTypes.createStructField(&quot;float&quot;, DataTypes.DoubleType, false) )); Dataset&lt;Row&gt; dataFrame = spark.createDataFrame(List.of( RowFactory.create(&quot;string&quot;, 1L, true, 1.5D) ), schema); fr.insee.vtl.model.Dataset sparkDataset = new SparkDataset(dataFrame);   ","version":"下一个","tagName":"h3"},{"title":"Spark 支持的其他数据格式​","type":1,"pageTitle":"Spark 模式 - 其它数据源","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/others#spark-支持的其他数据格式","content":" 请看 官方文档 ","version":"下一个","tagName":"h3"},{"title":"Trevas","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/introduction","content":"","keywords":"","version":"下一个"},{"title":"什么是 VTL？​","type":1,"pageTitle":"Trevas","url":"/Trevas/zh-CN/introduction#什么是-vtl","content":" 面向统计学家的商业、逻辑级和用户友好的语言依赖于标准结构元数据可互操作，与平台无关函数式语言，支持数据沿袭以获得更多洞察力和可重复性  ","version":"下一个","tagName":"h2"},{"title":"Trevas 生态系统​","type":1,"pageTitle":"Trevas","url":"/Trevas/zh-CN/introduction#trevas-生态系统","content":"   Trevas JupyterTrevas LabTrevas Lab APITrevas Batch  ","version":"下一个","tagName":"h2"},{"title":"技术说明​","type":1,"pageTitle":"Trevas","url":"/Trevas/zh-CN/introduction#技术说明","content":" Trevas 项目在各种运行环境下提供 VTL 引擎，包括 Java 11 引擎和 Apache Spark引擎.  Trevas 的引擎是基于JSR 223规范，该规范描述了 Java 中脚本语言的使用 ","version":"下一个","tagName":"h2"},{"title":"Spark 模式","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/developer-guide/spark-mode","content":"","keywords":"","version":"下一个"},{"title":"SparkDataset​","type":1,"pageTitle":"Spark 模式","url":"/Trevas/zh-CN/developer-guide/spark-mode#sparkdataset","content":" Les datasets SparkDataset permettent de représenter les tables statistiques dans une application Java utilisant Trevas en mode Spark.  ","version":"下一个","tagName":"h3"},{"title":"Importer le module Spark de Trevas​","type":1,"pageTitle":"Spark 模式","url":"/Trevas/zh-CN/developer-guide/spark-mode#importer-le-module-spark-de-trevas","content":" &lt;dependency&gt; &lt;groupId&gt;fr.insee.trevas&lt;/groupId&gt; &lt;artifactId&gt;vtl-spark&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt; &lt;/dependency&gt;   ","version":"下一个","tagName":"h3"},{"title":"Session Spark​","type":1,"pageTitle":"Spark 模式","url":"/Trevas/zh-CN/developer-guide/spark-mode#session-spark","content":" Afin d'exécuter du VTL via Trevas en mode Spark, il faut instancier une session Spark.  La session peut être :  locale (exécution sur le serveur Java)statique (exécution sur une instance Spark préalablement installée sur un serveur)dynamique (exécution dynamique au sein d'un cluster Kubernetes)  SparkSession spark = SparkSession.builder().master(&quot;local&quot;).getOrCreate();   ","version":"下一个","tagName":"h3"},{"title":"Exemple​","type":1,"pageTitle":"Spark 模式","url":"/Trevas/zh-CN/developer-guide/spark-mode#exemple","content":" ScriptEngine engine = new ScriptEngineManager().getEngineByName(&quot;vtl&quot;); Bindings bindings = new SimpleBindings(); SparkDataset dataset = new SparkDataset(spark.read().parquet(&quot;folder_path&quot;)); bindings.put(&quot;myDataset&quot;, dataset); engine.getContext().setBindings(bindings, ScriptContext.ENGINE_SCOPE); engine.put(&quot;$vtl.engine.processing_engine_names&quot;, &quot;spark&quot;); engine.put(&quot;$vtl.spark.session&quot;, spark); String script = &quot;res := myDataset[filter var3 &gt; 6];&quot;; try { engine.eval(script); } catch (ScriptException e) { e.printStackTrace(); } Bindings outputBindings = engine.getContext().getBindings(ScriptContext.ENGINE_SCOPE); SparkDataset result = (SparkDataset) outputBindings.get(&quot;res&quot;); // Ensure direct resolution because of spark lazy mechanism (performance warning!) InMemoryDataset imResult = new InMemoryDataset( result.getDataPoints(), result.getDataStructure() );   ","version":"下一个","tagName":"h3"},{"title":"Exécution distribuée​","type":1,"pageTitle":"Spark 模式","url":"/Trevas/zh-CN/developer-guide/spark-mode#exécution-distribuée","content":" Que ce soit en mode statique ou dynamique, l'éxecution distribuée des traitements nécessite que les exécuteurs instanciés par le master soient en capacité de résoudre les traitements VTL.  Il faut alors fournir les jar Trevas aux exécuteurs via l'options spark.jars de l'objet SparkConf :  SparkSession.Builder sparkBuilder = SparkSession.builder() SparkConf conf = new SparkConf(); conf.set(&quot;spark.jars&quot;, String.join(&quot;,&quot;, &quot;/vtl-spark.jar&quot;, &quot;/vtl-model.jar&quot;, &quot;/vtl-parser.jar&quot;, &quot;/vtl-engine.jar&quot;, )); sparkBuilder.config(conf); ... SparkSession spark = sparkBuilder.getOrCreate();   ","version":"下一个","tagName":"h3"},{"title":"Exécution dans un cluster Kubernetes​","type":1,"pageTitle":"Spark 模式","url":"/Trevas/zh-CN/developer-guide/spark-mode#exécution-dans-un-cluster-kubernetes","content":" De nombreuses options sont détaillées dans la documentation officielle  Parmi celles-ci, une option est particulièrement importante : l'image Docker qui permettra au exécuteurs de résoudre les traitements VTL.  Une image à façon est disponible ici.  SparkSession.Builder sparkBuilder = SparkSession.builder() SparkConf conf = new SparkConf(); conf.set(&quot;spark.kubernetes.container.image&quot;, &quot;inseefrlab/spark-hadoop:trevas-0.4.7-spark-3.2.1-hadoop-3.3.1-postgresql-42.3.3-postgis-2021.1.0&quot;); conf.set(&quot;spark.kubernetes.container.pullPolicy&quot;, &quot;IfNotPresent&quot;); sparkBuilder.config(conf); sparkBuilder.master(&quot;k8s://...&quot;) ... SparkSession spark = sparkBuilder.getOrCreate();  ","version":"下一个","tagName":"h3"},{"title":"prov","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/introduction/modules/prov","content":"prov","keywords":"","version":"下一个"},{"title":"Spark 模式 - Parquet 数据源","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/parquet","content":"","keywords":"","version":"下一个"},{"title":"元数据​","type":1,"pageTitle":"Spark 模式 - Parquet 数据源","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/parquet#元数据","content":" Parquet 数据集的元数据是从 parquet 的架构中推断出来的  数据类型​  Trevas 引擎负责在 Parquet 数据类型和 Trevas 支持的数据类型之间进行转换。  数据角色​  VTL 的数据角色由 Trevas 引擎添加到 Parquet 数据集中，方法是向每个字段描述添加元数据“vtlRole”。 在默认情况下，所有 Parquet 数据集中没有角色的列将在 Trevas 中具有角色“MEASURE”。  VTL 允许数据角色在脚本中进行演变 (请看 这里)  ","version":"下一个","tagName":"h3"},{"title":"读取​","type":1,"pageTitle":"Spark 模式 - Parquet 数据源","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/parquet#读取","content":" Dataset&lt;Row&gt; sparkDataset = spark.read().parquet(&quot;folder_path&quot;); SparkDataset dataset = new SparkDataset(sparkDataset);   ","version":"下一个","tagName":"h3"},{"title":"写入​","type":1,"pageTitle":"Spark 模式 - Parquet 数据源","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/parquet#写入","content":" // Trevas Spark Dataset SparkDataset dataset = ...; // Spark Dataset Dataset&lt;Row&gt; sparkDataset = dataset.getSparkDataset(); sparkDataset.write() .mode(SaveMode.Overwrite) .parquet(&quot;folder_path&quot;);  ","version":"下一个","tagName":"h3"},{"title":"VTL CSV","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/modules/csv","content":"VTL CSV Tools for the use of CSV data sources.","keywords":"","version":"下一个"},{"title":"VTL引擎","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/modules/engine","content":"VTL引擎 运行引擎及其基本的 Java 实现。","keywords":"","version":"下一个"},{"title":"VTL Jackson模块","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/modules/jackson","content":"VTL Jackson模块 数据集 JSON 序列化和反序列化模块","keywords":"","version":"下一个"},{"title":"VTL 数据模型模块","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/modules/model","content":"VTL 数据模型模块 定义引擎中使用的对象的模型","keywords":"","version":"下一个"},{"title":"VTL 语言解析模块","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/modules/parser","content":"VTL 语言解析模块 VTL 语言解析模块是基于VTL 2.1 形式语法，由Antlr生成的语言解析器。","keywords":"","version":"下一个"},{"title":"Trevas 模组","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/modules","content":"Trevas 模组 VTL Engine Moteur d'exécution et son implémentation Java de base VTL Parser Parseur généré par Antlr à partir de la grammaire formelle de VTL 2.1 VTL Spark Exécution de transformations VTL par Spark VTL Model Modèle définissant les objets utilisés dans le moteur VTL JDBC Utilisation de sources de données SQL VTL Jackson Sérialisation / désérialisation JSON de jeux de données VTL CSV Tools for the use of CSV data sources VTL SDMX Tools for the use of SDMX metadata VTL Provenance Tools for producing lineage metadata","keywords":"","version":"下一个"},{"title":"VTL JDBC模块","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/modules/jdbc","content":"VTL JDBC模块 该模块用于处理 SQL 数据源。","keywords":"","version":"下一个"},{"title":"VTL Spark模块","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/modules/spark","content":"VTL Spark模块 Spark 模块可以用Spark执行 VTL 的数据转换.","keywords":"","version":"下一个"},{"title":"Trevas releases","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/releases","content":"Trevas releases v1.x.x v2.x.x","keywords":"","version":"下一个"},{"title":"VTL SDMX","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/modules/sdmx","content":"VTL SDMX Tools for the use of SDMX metadata.","keywords":"","version":"下一个"},{"title":"Release 1.x.x","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/releases/1.x.x","content":"","keywords":"","version":"下一个"},{"title":"版本 1.12.0 - 08/25/25​","type":1,"pageTitle":"Release 1.x.x","url":"/Trevas/zh-CN/releases/1.x.x#版本-1120---082525","content":" (请参阅技术发布 Github)  改进 vtl-prov 模块  ","version":"下一个","tagName":"h2"},{"title":"版本 1.11.0 - 08/03/25​","type":1,"pageTitle":"Release 1.x.x","url":"/Trevas/zh-CN/releases/1.x.x#版本-1110---080325","content":" (请参阅技术发布 Github)  实施枢轴支持布尔值和日期作为聚合中的最小值/最大值  ","version":"下一个","tagName":"h2"},{"title":"版本 1.10.0 - 07/21/25​","type":1,"pageTitle":"Release 1.x.x","url":"/Trevas/zh-CN/releases/1.x.x#版本-1100---072125","content":" (请参阅技术发布 Github)  修复 Spark 数据集构建优化问题修复验证的来源问题  ","version":"下一个","tagName":"h2"},{"title":"版本 1.9.0 - 05/26/25​","type":1,"pageTitle":"Release 1.x.x","url":"/Trevas/zh-CN/releases/1.x.x#版本-190---052625","content":" (请参阅技术发布 Github)  需要 Java 17 或更高版本  ","version":"下一个","tagName":"h2"},{"title":"版本 1.8.0 - 11/12/24​","type":1,"pageTitle":"Release 1.x.x","url":"/Trevas/zh-CN/releases/1.x.x#版本-180---111224","content":" (请参阅技术发布 Github)  改进 Provenance 模块  ","version":"下一个","tagName":"h2"},{"title":"版本 1.7.0 - 10/09/24​","type":1,"pageTitle":"Release 1.x.x","url":"/Trevas/zh-CN/releases/1.x.x#版本-170---100924","content":" (请参阅技术发布 Github)  升级至VTL 2.1版本  ","version":"下一个","tagName":"h2"},{"title":"版本 1.6.0 - 10/07/24​","type":1,"pageTitle":"Release 1.x.x","url":"/Trevas/zh-CN/releases/1.x.x#版本-160---100724","content":" (请参阅技术发布 Github)  添加出处模块  ","version":"下一个","tagName":"h2"},{"title":"版本 1.5.0 - 06/28/24​","type":1,"pageTitle":"Release 1.x.x","url":"/Trevas/zh-CN/releases/1.x.x#版本-150---062824","content":" (请参阅技术发布 Github)  需要 Java 8 或更高版本  ","version":"下一个","tagName":"h2"},{"title":"版本 1.4.1 - 06/25/24​","type":1,"pageTitle":"Release 1.x.x","url":"/Trevas/zh-CN/releases/1.x.x#版本-141---062524","content":" (请参阅技术发布 Github)  修复 VTL SDMX 模块可见性  ","version":"下一个","tagName":"h2"},{"title":"版本 1.4.0 - 06/07/24​","type":1,"pageTitle":"Release 1.x.x","url":"/Trevas/zh-CN/releases/1.x.x#版本-140---060724","content":" (请参阅技术发布 Github)  支持时间运算符添加VTL CSV模块添加VTL SDMX模块  ","version":"下一个","tagName":"h2"},{"title":"版本 1.3.0 - 01/17/24​","type":1,"pageTitle":"Release 1.x.x","url":"/Trevas/zh-CN/releases/1.x.x#版本-130---011724","content":" (请参阅技术发布 Github)  修复应用于数据集时分析运算符的实现  ","version":"下一个","tagName":"h2"},{"title":"版本 1.2.0 - 22/11/23​","type":1,"pageTitle":"Release 1.x.x","url":"/Trevas/zh-CN/releases/1.x.x#版本-120---221123","content":" (请参阅技术发布 Github)  Java 17 支持持久分配 (&lt;-)  ","version":"下一个","tagName":"h2"},{"title":"版本 1.1.1 - 09/09/23​","type":1,"pageTitle":"Release 1.x.x","url":"/Trevas/zh-CN/releases/1.x.x#版本-111---090923","content":" (请参阅技术发布 Github)  check_hierarchy 修复：序列化和返回空数据集的可能性  ","version":"下一个","tagName":"h2"},{"title":"版本 1.1.0 - 09/01/23​","type":1,"pageTitle":"Release 1.x.x","url":"/Trevas/zh-CN/releases/1.x.x#版本-110---090123","content":" (请参阅技术发布 Github)  验证: define hierarchical ruleset, check_hierarchy  ","version":"下一个","tagName":"h2"},{"title":"版本 1.0.2 - 06/30/23​","type":1,"pageTitle":"Release 1.x.x","url":"/Trevas/zh-CN/releases/1.x.x#版本-102---063023","content":" (请参阅技术发布 Github)  使用 2 种方法分割引擎方法注册：registerMethod 和 registerGlobalMethod使用 null 修复 in / not_in 错误  ","version":"下一个","tagName":"h2"},{"title":"版本 1.0.1 - 05/23/23​","type":1,"pageTitle":"Release 1.x.x","url":"/Trevas/zh-CN/releases/1.x.x#版本-101---052323","content":" (请参阅技术发布 Github)  修复 check_datapoint 实现中的 Spark 序列化问题  ","version":"下一个","tagName":"h2"},{"title":"版本 1.0.0 - 05/12/23​","type":1,"pageTitle":"Release 1.x.x","url":"/Trevas/zh-CN/releases/1.x.x#版本-100---051223","content":" (请参阅技术发布 Github)  会籍: #验证: check, check_datapoint应用于数据集的运算符 (ceil(ds), ds1 &lt; ds2, mod(ds, 5), ...) ","version":"下一个","tagName":"h2"},{"title":"Spark mode - SDMX source","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/sdmx","content":"","keywords":"","version":"下一个"},{"title":"buildStructureFromSDMX3 utility​","type":1,"pageTitle":"Spark mode - SDMX source","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/sdmx#buildstructurefromsdmx3-utility","content":" TrevasSDMXUtils.buildStructureFromSDMX3 allows to obtain a Trevas DataStructure.  Providing corresponding data, you can build a Trevas Dataset.  Structured.DataStructure structure = TrevasSDMXUtils.buildStructureFromSDMX3(&quot;path/sdmx_file.xml&quot;, &quot;STRUCT_ID&quot;); SparkDataset ds = new SparkDataset( spark.read() .option(&quot;header&quot;, &quot;true&quot;) .option(&quot;delimiter&quot;, &quot;;&quot;) .option(&quot;quote&quot;, &quot;\\&quot;&quot;) .csv(&quot;path&quot;), structure );   ","version":"下一个","tagName":"h3"},{"title":"SDMXVTLWorkflow object​","type":1,"pageTitle":"Spark mode - SDMX source","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/sdmx#sdmxvtlworkflow-object","content":" The SDMXVTLWorkflow constructor takes 3 arguments:  a ScriptEngine (Trevas or another)a ReadableDataLocation to handle an SDMX messagea map of names / Datasets  SparkSession.builder() .appName(&quot;test&quot;) .master(&quot;local&quot;) .getOrCreate(); ScriptEngineManager mgr = new ScriptEngineManager(); ScriptEngine engine = mgr.getEngineByExtension(&quot;vtl&quot;); engine.put(VtlScriptEngine.PROCESSING_ENGINE_NAMES, &quot;spark&quot;); ReadableDataLocation rdl = new ReadableDataLocationTmp(&quot;src/test/resources/DSD_BPE_CENSUS.xml&quot;); SDMXVTLWorkflow sdmxVtlWorkflow = new SDMXVTLWorkflow(engine, rdl, Map.of());   This object then allows you to activate the following 3 functions.  ","version":"下一个","tagName":"h3"},{"title":"SDMXVTLWorkflow run function - Preview mode​","type":1,"pageTitle":"Spark mode - SDMX source","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/sdmx#sdmxvtlworkflow-run-function---preview-mode","content":" The run function can easily be called in a preview mode, without attached data.  ScriptEngineManager mgr = new ScriptEngineManager(); ScriptEngine engine = mgr.getEngineByExtension(&quot;vtl&quot;); engine.put(VtlScriptEngine.PROCESSING_ENGINE_NAMES, &quot;spark&quot;); ReadableDataLocation rdl = new ReadableDataLocationTmp(&quot;src/test/resources/DSD_BPE_CENSUS.xml&quot;); SDMXVTLWorkflow sdmxVtlWorkflow = new SDMXVTLWorkflow(engine, rdl, Map.of()); // instead of using TrevasSDMXUtils.buildStructureFromSDMX3 and data sources // to build Trevas Datasets, sdmxVtlWorkflow.getEmptyDatasets() // will handle SDMX message structures to produce Trevas Datasets // with metadata defined in this message, and adding empty data Map&lt;String, Dataset&gt; emptyDatasets = sdmxVtlWorkflow.getEmptyDatasets(); engine.getBindings(ScriptContext.ENGINE_SCOPE).putAll(emptyDatasets); Map&lt;String, PersistentDataset&gt; result = sdmxVtlWorkflow.run();   The preview mode allows to check the conformity of the SDMX file and the metadata of the output datasets.  ","version":"下一个","tagName":"h3"},{"title":"SDMXVTLWorkflow run function​","type":1,"pageTitle":"Spark mode - SDMX source","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/sdmx#sdmxvtlworkflow-run-function","content":" Once an SDMXVTLWorkflow is built, it is easy to run the VTL validations and transformations defined in the SDMX file.  Structured.DataStructure structure = TrevasSDMXUtils.buildStructureFromSDMX3(&quot;path/sdmx_file.xml&quot;, &quot;ds1&quot;); SparkDataset ds1 = new SparkDataset( spark.read() .option(&quot;header&quot;, &quot;true&quot;) .option(&quot;delimiter&quot;, &quot;;&quot;) .option(&quot;quote&quot;, &quot;\\&quot;&quot;) .csv(&quot;path/data.csv&quot;), structure ); ScriptEngineManager mgr = new ScriptEngineManager(); ScriptEngine engine = mgr.getEngineByExtension(&quot;vtl&quot;); engine.put(VtlScriptEngine.PROCESSING_ENGINE_NAMES, &quot;spark&quot;); Map&lt;String, Dataset&gt; inputs = Map.of(&quot;ds1&quot;, ds1); ReadableDataLocation rdl = new ReadableDataLocationTmp(&quot;path/sdmx_file.xml&quot;); SDMXVTLWorkflow sdmxVtlWorkflow = new SDMXVTLWorkflow(engine, rdl, inputs); Map&lt;String, PersistentDataset&gt; bindings = sdmxVtlWorkflow.run();   As a result, one will receive all the dataset defined as persistent in the TransformationSchemes definition.  ","version":"下一个","tagName":"h3"},{"title":"SDMXVTLWorkflow getTransformationsVTL function​","type":1,"pageTitle":"Spark mode - SDMX source","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/sdmx#sdmxvtlworkflow-gettransformationsvtl-function","content":" Gets the VTL code corresponding to the SDMX TransformationSchemes definition.  SDMXVTLWorkflow sdmxVtlWorkflow = new SDMXVTLWorkflow(engine, rdl, Map.of()); String vtl = sdmxVtlWorkflow.getTransformationsVTL();   ","version":"下一个","tagName":"h3"},{"title":"SDMXVTLWorkflow getRulesetsVTL function​","type":1,"pageTitle":"Spark mode - SDMX source","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/sdmx#sdmxvtlworkflow-getrulesetsvtl-function","content":" Gets the VTL code corresponding to the SDMX TransformationSchemes definition.  SDMXVTLWorkflow sdmxVtlWorkflow = new SDMXVTLWorkflow(engine, rdl, Map.of()); String dprs = sdmxVtlWorkflow.getRulesetsVTL();   ","version":"下一个","tagName":"h3"},{"title":"Troubleshooting​","type":1,"pageTitle":"Spark mode - SDMX source","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/sdmx#troubleshooting","content":" ","version":"下一个","tagName":"h2"},{"title":"Hadoop client​","type":1,"pageTitle":"Spark mode - SDMX source","url":"/Trevas/zh-CN/developer-guide/spark-mode/data-sources/sdmx#hadoop-client","content":" The integration of vtl-modules with hadoop-client can cause dependency issues.  It was noted that com.fasterxml.woodstox.woodstox-core is imported by hadoop-client, with an incompatible version for a vtl-sdmx sub-dependency.  A way to fix is to exclude com.fasterxml.woodstox.woodstox-core dependency from hadoop-client and import a newest version in your pom.xml:  &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;3.3.4&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.fasterxml.woodstox&lt;/groupId&gt; &lt;artifactId&gt;woodstox-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.woodstox&lt;/groupId&gt; &lt;artifactId&gt;woodstox-core&lt;/artifactId&gt; &lt;version&gt;6.5.1&lt;/version&gt; &lt;/dependency&gt;  ","version":"下一个","tagName":"h3"},{"title":"Release 2.x.x","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/releases/2.x.x","content":"","keywords":"","version":"下一个"},{"title":" 版本 2.0.0 - 09/28/25​","type":1,"pageTitle":"Release 2.x.x","url":"/Trevas/zh-CN/releases/2.x.x#-版本-200---092825","content":" (请参阅技术发布 Github)  实现 DAG 并在执行前重新排序 VTL 语句修复 vtl-prov 和 vtl-sdmx 问题修复比较问题修复绑定初始化以避免变量重用 ","version":"下一个","tagName":"h2"},{"title":"Trevas VTL coverage","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/coverage","content":"Trevas VTL coverage General purpose operators Join operators String operators Numeric operators Comparison operators Boolean operators Time operators Set operators Hierarchical aggregation Aggregate operators Analytic operators Data validation operators Conditional operators Clause operators","keywords":"","version":"下一个"},{"title":"VTL - Aggregate operators","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/coverage/aggregate-operators","content":"VTL - Aggregate operators Name\tSymbol\tInMemory\tSparkAggregate invocation ✔️\t✔️ Counting the number of data points\tcount\t✔️\t✔️ Minimum value\tmin\t✔️\t✔️ Maximum value\tmax\t✔️\t✔️ Median value\tmedian\t✔️\t✔️ Sum\tsum\t✔️\t✔️ Average value\tavg\t✔️\t✔️ Population standard deviation\tstddev_pop\t✔️\t✔️ Sample standard deviation\tstddev_samp\t✔️\t✔️ Population variance\tvar_pop\t✔️\t✔️ Sample variance\tvar_samp\t✔️\t✔️","keywords":"","version":"下一个"},{"title":"VTL - Boolean operators","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/coverage/boolean-operators","content":"VTL - Boolean operators Name\tSymbol\tBoolean\tComponent\tDatasetLogical conjunction\tand\t✔️\t✔️\t✔️ Logical disjunction\tor\t✔️\t✔️\t✔️ Exclusive disjunction\txor\t✔️\t✔️\t✔️ Logical negation\tnot\t✔️\t✔️\t✔️","keywords":"","version":"下一个"},{"title":"VTL - Analytic operators","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/coverage/analytic-operators","content":"VTL - Analytic operators Name\tSymbol\tInMemory\tSparkAnalytic invocation ❌\t✔️ First value\tfirst_value\t❌\t✔️ Last value\tlast_value\t❌\t✔️ Lag\tlag\t❌\t✔️ lead\tlead\t❌\t✔️ Rank\trank\t❌\t✔️ Ratio to report\tratio_to_report\t❌\t✔️","keywords":"","version":"下一个"},{"title":"VTL - Clause operators","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/coverage/clause-operators","content":"VTL - Clause operators Name\tSymbol\tInMemory\tSparkFiltering Data Points\tfilter\t✔️\t✔️ Calculation of a Component\tcalc\t✔️\t✔️ Aggregation\taggr\t❌\t✔️ Maintaining Components\tkeep\t✔️\t✔️ Removal of Components\tdrop\t✔️\t✔️ Change of Component name\trename\t✔️\t✔️ Pivoting\tpivot\t❌\t✔️ Unpivoting\tunpivot\t❌\t❌ Subspace\tsub\t❌\t❌","keywords":"","version":"下一个"},{"title":"VTL - Conditional operators","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/coverage/conditional-operators","content":"VTL - Conditional operators Name\tSymbol\tBoolean\tComponent\tDatasetIf Then Else\tif-then-else\t✔️\t✔️\t✔️ Case\tcase\t✔️\t✔️\t✔️ Nvl\tnvl\t✔️\t✔️\t✔️","keywords":"","version":"下一个"},{"title":"VTL - Comparison operators","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/coverage/comparison-operators","content":"VTL - Comparison operators Name\tSymbol\tScalar\tComponent\tDatasetEqual to\t=\t✔️\t✔️\t✔️ Not equal to\t&lt;&gt;\t✔️\t✔️\t✔️ Greater than\t&gt;\t✔️\t✔️\t✔️ Less than\t&lt;\t✔️\t✔️\t✔️ Greater or equal than\t&gt;=\t✔️\t✔️\t✔️ Less or equal than\t&lt;=\t✔️\t✔️\t✔️ Between\tbetween\t✔️\t✔️\t✔️ Element of\tin / not_in\t✔️\t✔️\t✔️ Match characters\tmatch_characters\t✔️\t✔️\t✔️ Is null\tisnull\t✔️\t✔️\t✔️ Exists in\texists_in\tN/A\tN/A\t❌","keywords":"","version":"下一个"},{"title":"VTL - General purpose operators","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/coverage/general-operators","content":"VTL - General purpose operators Name\tSymbol\tSupportedParentheses\t( )\t✔️ Persistent assignment\t&lt;-\t✔️ Temporary assignment\t:=\t✔️ Membership\t#\t✔️ User-defined operator call ✔️ Evaluation of an external routine\teval\t❌ Type conversion (string, integer, double, boolean)\tcast\t✔️ Type conversion (others)\tcast\t❌","keywords":"","version":"下一个"},{"title":"VTL - Data validation operators","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/coverage/data-validation-operators","content":"VTL - Data validation operators Name\tSymbol\tSupportedCheck datapoint\tcheck_datapoint\t✔️ Check hierarchy\tcheck_hierarchy\t✔️ Check\tcheck\t✔️","keywords":"","version":"下一个"},{"title":"VTL - Join operators","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/coverage/join-operators","content":"VTL - Join operators Name\tSymbol\tInMemory\tSparkJoin\tinner_join, left_join, full_join, cross_join\t✔️\t✔️","keywords":"","version":"下一个"},{"title":"VTL - Hierarchical aggregation","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/coverage/hierarchical-aggregation","content":"VTL - Hierarchical aggregation Name\tSymbol\tInMemory\tSparkHierarchical roll-up\thierarchy\t❌\t❌","keywords":"","version":"下一个"},{"title":"VTL - String operators","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/coverage/string-operators","content":"VTL - String operators Name\tSymbol\tString\tComponent\tDatasetString concatenation\t||\t✔️\t✔️\t✔️ Whitespace removal\ttrim, rtrim, ltrim\t✔️\t✔️\t✔️ Character case conversion\tupper/lower\t✔️\t✔️\t✔️ Sub-string extraction\tsubstr\t✔️\t✔️\t✔️ String pattern replacement\treplace\t✔️\t✔️\t✔️ String pattern location\tinstr\t✔️\t✔️\t✔️ String length\tlength\t✔️\t✔️\t✔️","keywords":"","version":"下一个"},{"title":"VTL - Set operators","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/coverage/set-operators","content":"VTL - Set operators Name\tSymbol\tInMemory\tSparkUnion\tunion\t❌\t✔️ Intersection\tintersect\t❌\t❌ Set difference\tsetdiff\t❌\t❌ Symmetric difference\tsymdiff\t❌\t❌","keywords":"","version":"下一个"},{"title":"VTL","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/vtl","content":"","keywords":"","version":"下一个"},{"title":"Documentation​","type":1,"pageTitle":"VTL","url":"/Trevas/zh-CN/user-guide/vtl#documentation","content":" La documentation de VTL est accessible sur le site officiel SDMX :  Manuel utilisateurManuel de référence  ","version":"下一个","tagName":"h2"},{"title":"Jeux de données​","type":1,"pageTitle":"VTL","url":"/Trevas/zh-CN/user-guide/vtl#jeux-de-données","content":" Les jeux de données VTL doivent être décrit par des métadonnées. Les différentes colonnes ont un type et un role.  Par défaut, dans Trevas, une colonne sans type ni rôle se verra affecter le type string et le role measure.  L'utilisateur aura ensuite la possibilité de muter ces attributs au sein de son script, via les opérateurs calc et cast notamment.  ","version":"下一个","tagName":"h2"},{"title":"Exemple simple​","type":1,"pageTitle":"VTL","url":"/Trevas/zh-CN/user-guide/vtl#exemple-simple","content":" En considérant ds_1 définit comme suit :  \tid_1\tid_2\tme_1type\tstring\tstring\tstring role\tidentifier\tidentifier\tmeasure  id_1\tid_2\tme_1&quot;75001&quot;\t&quot;75&quot;\t&quot;10&quot; &quot;75002&quot;\t&quot;75&quot;\t&quot;100&quot; &quot;70001&quot;\t&quot;70&quot;\t&quot;5&quot; &quot;70002&quot;\t&quot;70&quot;\t&quot;70&quot;  Pour obtenir la somme de me_1 par id_2, il faut que seul id_2 ait pour rôle identifier et que le type de me_1 soit integer. On applique donc le script suivant :  ds_2 := ds_1[calc measure id_2 := id_2, me_1 := cast(me_1, integer)];   ds_2 est alors :  \tid_1\tid_2\tme_1type\tstring\tstring\tinteger role\tmeasure\tidentifier\tmeasure  id_1\tid_2\tme_1&quot;75001&quot;\t&quot;75&quot;\t10 &quot;75002&quot;\t&quot;75&quot;\t100 &quot;70001&quot;\t&quot;70&quot;\t5 &quot;70002&quot;\t&quot;70&quot;\t70  On peut dorénavant appliquer l'opérateur aggr :  ds_3 := ds_2[aggr sum_me_1 := sum(me_1) group by id_2];   Et obtenir ds_3 :  \tid_2\tsum_me_1type\tstring\tinteger role\tidentifier\tmeasure  id_2\tme_1&quot;75&quot;\t110 &quot;70&quot;\t75 ","version":"下一个","tagName":"h2"},{"title":"VTL - Numeric operators","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/coverage/numeric-operators","content":"VTL - Numeric operators Name\tSymbol\tNumber\tComponent\tDatasetUnary plus\t+\t✔️\t✔️\t✔️ Unary minus\t-\t✔️\t✔️\t✔️ Addition\t+\t✔️\t✔️\t✔️ Subtraction\t-\t✔️\t✔️\t✔️ Multiplication\t*\t✔️\t✔️\t✔️ Division\t/\t✔️\t✔️\t✔️ Concatenation\t||\t✔️\t✔️\t✔️ Modulo\tmod\t✔️\t✔️\t✔️ Rounding\tround\t✔️\t✔️\t✔️ Truncation\ttrunc\t✔️\t✔️\t✔️ Ceiling\tceil\t✔️\t✔️\t✔️ Floor\tfloor\t✔️\t✔️\t✔️ Absolute value\tabs\t✔️\t✔️\t✔️ Exponential\texp\t✔️\t✔️\t✔️ Natural logarithm\tln\t✔️\t✔️\t✔️ Power\tpower\t✔️\t✔️\t✔️ Random\trandom\t✔️\t✔️\t✔️ Logarithm\tlog\t✔️\t✔️\t✔️ Square root\tsqrt\t✔️\t✔️\t✔️","keywords":"","version":"下一个"},{"title":"VTL - Time operators","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/coverage/time-operators","content":"VTL - Time operators Name\tSymbol\tTime_period\tComponent\tDatasetPeriod indicator\tperiod_indicator\t✔️\t✔️\t✔️ Fill time series\tfill_time_series\tN/A\tN/A\t❌ Flow to stock\tflow_to_stock\tN/A\tN/A\t✔️ Stock to flow\tstock_to_flow\tN/A\tN/A\t✔️ Time shift\ttimeshift\t✔️\t✔️\t✔️ Time aggregation\ttime_agg\tN/A\tN/A\t❌ Actual time\tcurrent_date\t✔️\tN/A\tN/A Subtract dates\tdatediff\t❌\t❌\t❌ Add to date\tdateadd\t❌\t❌\t❌ Year of a date\tyear\t❌\t❌\t❌ Month of a date\tmonth\t❌\t❌\t❌ Number of day within the year\tdayofyear\t❌\t❌\t❌ Number of day within the month\tdayofmonth\t❌\t❌\t❌ Convert days to years\tdaytoyear\t❌\t❌\t❌ Convert days to months\tdaytomonth\t❌\t❌\t❌ Convert duration in days\tyeartoday\t❌\t❌\t❌ Convert duration in months\tyeartomonth\t❌\t❌\t❌","keywords":"","version":"下一个"},{"title":"Trevas Jupyter","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/vtl/client-apps/jupyter","content":"","keywords":"","version":"下一个"},{"title":"Sources​","type":1,"pageTitle":"Trevas Jupyter","url":"/Trevas/zh-CN/user-guide/vtl/client-apps/jupyter#sources","content":" Github​  Trevas Jupyter  Docker Hub​  Trevas Jupyter  ","version":"下一个","tagName":"h3"},{"title":"Demo​","type":1,"pageTitle":"Trevas Jupyter","url":"/Trevas/zh-CN/user-guide/vtl/client-apps/jupyter#demo","content":" A video will be coming soon ","version":"下一个","tagName":"h3"},{"title":"Drop","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/vtl/sas-vtl/drop","content":"Drop Sas VTL","keywords":"","version":"下一个"},{"title":"Sas vs VTL examples","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/vtl/sas-vtl","content":"Sas vs VTL examples Keep Drop Rename","keywords":"","version":"下一个"},{"title":"Keep","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/vtl/sas-vtl/keep","content":"Keep Sas VTL","keywords":"","version":"下一个"},{"title":"Trevas Jupyter","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/vtl/client-apps/lab","content":"","keywords":"","version":"下一个"},{"title":"Sources​","type":1,"pageTitle":"Trevas Jupyter","url":"/Trevas/zh-CN/user-guide/vtl/client-apps/lab#sources","content":" Github​  Trevas Lab UITrevas LabTrevas Spark Hadoop  Docker Hub​  Trevas Lab UITrevas LabTrevas Spark Hadoop  ","version":"下一个","tagName":"h3"},{"title":"Demo​","type":1,"pageTitle":"Trevas Jupyter","url":"/Trevas/zh-CN/user-guide/vtl/client-apps/lab#demo","content":" A video will be coming soon ","version":"下一个","tagName":"h3"},{"title":"Trevas client apps","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/vtl/client-apps","content":"Trevas client apps Trevas Jupyter Kernel Jupyter Trevas Lab Application web","keywords":"","version":"下一个"},{"title":"Rename","type":0,"sectionRef":"#","url":"/Trevas/zh-CN/user-guide/vtl/sas-vtl/rename","content":"Rename Sas VTL","keywords":"","version":"下一个"}],"options":{"indexBaseUrl":true,"languages":["en","fr","no"],"id":"default"}}